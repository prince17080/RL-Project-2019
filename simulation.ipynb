{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "# tf.disable_v2_behavior()\n",
    "from sumolib import checkBinary\n",
    "import datetime\n",
    "import math\n",
    "import timeit\n",
    "import traci\n",
    "import random\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "\n",
    "import memory\n",
    "from model import *\n",
    "import traffic_generator\n",
    "\n",
    "# add SUMO_HOME to the environment variable 'PATH'\n",
    "if 'SUMO_HOME' in os.environ:\n",
    "    tools = os.path.join(os.environ['SUMO_HOME'], 'tools')\n",
    "    sys.path.append(tools)\n",
    "else:\n",
    "    sys.exit(\"please declare environment variable 'SUMO_HOME'\")\n",
    "\n",
    "\n",
    "SHOW_GUI = False\n",
    "BATCH_FLAG = False\n",
    "VERSION = 1\n",
    "PATH = \"./figures/version\" + str(VERSION)\n",
    "STATE_SPACE = 80\n",
    "ACTION_SPACE = 4\n",
    "MEMORY = 100000 # to store the data elements/samples\n",
    "BATCH = 100\n",
    "NO_OF_EPISODES = 10\n",
    "NO_OF_CARS = 1000\n",
    "\n",
    "PHASE_NS_GREEN = 0  # action 0 code 00\n",
    "PHASE_NS_YELLOW = 1\n",
    "PHASE_NSL_GREEN = 2  # action 1 code 01\n",
    "PHASE_NSL_YELLOW = 3\n",
    "PHASE_EW_GREEN = 4  # action 2 code 10\n",
    "PHASE_EW_YELLOW = 5\n",
    "PHASE_EWL_GREEN = 6  # action 3 code 11\n",
    "PHASE_EWL_YELLOW = 7\n",
    "\n",
    "# below values are in seconds\n",
    "GREEN_TIME = 10\n",
    "YELLOW_TIME = 4\n",
    "MAX_STEPS_PER_EPS = 5400\n",
    "\n",
    "\n",
    "class Simulator:\n",
    "    def __init__(self, _traffic_generator, _niterations, _gamma):\n",
    "        self._traffic_generator = _traffic_generator\n",
    "        self.niterations = _niterations\n",
    "        self._gamma = _gamma\n",
    "        self.epsilon = 1\n",
    "        self.seed = 0\n",
    "        self.steps = 0\n",
    "        self._waiting_times = {}\n",
    "        self._green_duration = GREEN_TIME\n",
    "        self._yellow_duration = YELLOW_TIME\n",
    "        self._sum_intersection_queue = 0\n",
    "        self._old_waiting_time = 0\n",
    "        self._reward_store = []\n",
    "        self._cumulative_wait_store = []\n",
    "        self._avg_intersection_queue_store = []\n",
    "\n",
    "    def _simulate(self, Session, memory, model1, model2, steps_todo):\n",
    "        if (self.steps + steps_todo) >= MAX_STEPS_PER_EPS:  # do not do more steps than the maximum number of steps\n",
    "            steps_todo = MAX_STEPS_PER_EPS - self.steps\n",
    "        self.steps = self.steps + steps_todo  # update the step counter\n",
    "        while steps_todo > 0:\n",
    "            traci.simulationStep()  # simulate 1 step in sumo\n",
    "            self._replay(memory, model1, model2, Session)  # training\n",
    "            steps_todo -= 1\n",
    "            intersection_queue = self._get_stats()\n",
    "            self._sum_intersection_queue += intersection_queue\n",
    "    \n",
    "    def run(self, Session, memory, model1, model2, epsilon, port, sumo_cmd):\n",
    "        # first, generate the route file for this simulation and set up sumo\n",
    "        self._traffic_generator.generate_routefile(self.seed)\n",
    "        self.seed = (self.seed + 1)%5\n",
    "        traci.start(sumo_cmd, port)\n",
    "\n",
    "        # inits\n",
    "        self.steps = 0\n",
    "        tot_neg_reward = 0\n",
    "        old_total_wait = 0\n",
    "        self._waiting_times = {}\n",
    "        self._sum_intersection_queue = 0\n",
    "        old_action = random.randint(0, ACTION_SPACE-1)\n",
    "        while self.steps < MAX_STEPS_PER_EPS:\n",
    "\n",
    "            # get current state of the intersection\n",
    "            current_state = self._get_state()\n",
    "\n",
    "            # calculate reward of previous action: (change in cumulative waiting time between actions)\n",
    "            # waiting time = seconds waited by a car since the spawn in the environment, cumulated for every car in incoming lanes\n",
    "            \n",
    "            current_total_wait = self._get_waiting_times()\n",
    "            reward = old_total_wait - current_total_wait\n",
    "\n",
    "            if model2 == None:\n",
    "                action, reward = self.step_dqn(current_state, model1, epsilon, Session)\n",
    "            else:\n",
    "                action, reward = self.step_ddqn(current_state, model1, model2, epsilon, Session)\n",
    "\n",
    "\n",
    "            # saving the data into the memory\n",
    "            if self.steps != 0:\n",
    "                memory.add_data((old_state, old_action, reward, current_state))\n",
    "\n",
    "            # choose the light phase to activate, based on the current state of the intersection\n",
    "\n",
    "            # if the chosen phase is different from the last phase, activate the yellow phase\n",
    "            if self.steps != 0 and old_action != action:\n",
    "                self._set_yellow_phase(old_action)\n",
    "                self._simulate(Session, memory, model1, model2, self._yellow_duration)\n",
    "\n",
    "            # execute the phase selected before\n",
    "            self._set_green_phase(action)\n",
    "            self._simulate(Session, memory, model1, model2, self._green_duration)\n",
    "\n",
    "            # saving variables for later & accumulate reward\n",
    "            old_state = current_state\n",
    "            old_action = action\n",
    "            # old_total_wait = current_total_wait\n",
    "            if reward < 0:\n",
    "                tot_neg_reward += reward\n",
    "            self.steps += 1\n",
    "\n",
    "        self._save_stats(tot_neg_reward)\n",
    "        print(\"Total reward: {}, Eps: {}\".format(tot_neg_reward, epsilon))\n",
    "        traci.close()\n",
    "\n",
    "    def step_dqn(self, state, model, epsilon, Session):\n",
    "        e = random.random()\n",
    "        if e < epsilon:\n",
    "            action = random.randint(0, ACTION_SPACE-1)\n",
    "        else:\n",
    "            action = np.argmax(model.predict_one(state, Session))\n",
    "\n",
    "        current_waiting_time = self._get_waiting_times()\n",
    "        reward =  self._old_waiting_time - current_waiting_time\n",
    "        self._old_waiting_time = current_waiting_time\n",
    "\n",
    "        return action, reward\n",
    "\n",
    "    def step_ddqn(self, state, model1, model2, epsilon, Session):\n",
    "        e = random.random()\n",
    "        if e < epsilon:\n",
    "            action = random.randint(0, ACTION_SPACE-1)\n",
    "        else:\n",
    "            if(random.random()<0.5):\n",
    "                action = np.argmax(model1.predict_one(state, Session))\n",
    "            else:\n",
    "                action = np.argmax(model2.predict_one(state, Session))\n",
    "\n",
    "\n",
    "        current_waiting_time = self._get_waiting_times()\n",
    "        reward = self._old_waiting_time - current_waiting_time\n",
    "        self._old_waiting_time = current_waiting_time\n",
    "\n",
    "        return action, reward\n",
    "\n",
    "\n",
    "    def _get_waiting_times(self):\n",
    "        incoming_roads = [\"E2TL\", \"N2TL\", \"W2TL\", \"S2TL\"]\n",
    "        for veh_id in traci.vehicle.getIDList():\n",
    "            wait_time_car = traci.vehicle.getAccumulatedWaitingTime(veh_id)\n",
    "            road_id = traci.vehicle.getRoadID(veh_id)  # get the road id where the car is located\n",
    "            if road_id in incoming_roads:  # consider only the waiting times of cars in incoming roads\n",
    "                self._waiting_times[veh_id] = wait_time_car\n",
    "            else:\n",
    "                if veh_id in self._waiting_times:\n",
    "                    del self._waiting_times[veh_id]  # the car isnt in incoming roads anymore, delete his waiting time\n",
    "        total_waiting_time = sum(self._waiting_times.values())\n",
    "        return total_waiting_time\n",
    "\n",
    "    def _replay(self, memory, model1, model2, Session):\n",
    "        if(BATCH_FLAG == True):\n",
    "            batch = memory.get_batch(BATCH)\n",
    "            if len(batch) > 0:  # if there is at least 1 sample in the batch\n",
    "                if(model2==None):\n",
    "                    states = np.array([val[0] for val in batch])  # extract states from the batch\n",
    "                    next_states = np.array([val[3] for val in batch])  # extract next states from the batch\n",
    "\n",
    "                    # prediction\n",
    "                    q_s_a = model1.predict_batch(states, Session)  # predict Q(state), for every sample\n",
    "                    q_s_a_d = model1.predict_batch(next_states, Session)  # predict Q(next_state), for every sample\n",
    "\n",
    "                    # setup training arrays\n",
    "                    x = np.zeros((len(batch), STATE_SPACE))\n",
    "                    y = np.zeros((len(batch), ACTION_SPACE))\n",
    "\n",
    "                    for i, b in enumerate(batch):\n",
    "                        state, action, reward, next_state = b[0], b[1], b[2], b[3]  # extract data from one sample\n",
    "                        current_q = q_s_a[i]  # get the Q(state) predicted before\n",
    "\n",
    "\n",
    "                        current_q[action] = reward + self._gamma * np.amax(q_s_a_d[i])  # update Q(state, action)\n",
    "                        x[i] = state\n",
    "                        y[i] = current_q  # Q(state) that includes the updated action value\n",
    "\n",
    "                    model1.train_batch(Session, x, y)  # train the N\n",
    "                \n",
    "                else:\n",
    "                    if(random.random()<=0.5):\n",
    "                        states = np.array([val[0] for val in batch])  # extract states from the batch\n",
    "                        next_states = np.array([val[3] for val in batch])  # extract next states from the batch\n",
    "\n",
    "                        # prediction\n",
    "                        q_s_a = model1.predict_batch(states, Session)  # predict Q(state), for every sample\n",
    "                        q_s_a_d = model2.predict_batch(next_states, Session)  # predict Q(next_state), for every sample\n",
    "\n",
    "                        # setup training arrays\n",
    "                        x = np.zeros((len(batch),  STATE_SPACE))\n",
    "                        y = np.zeros((len(batch), ACTION_SPACE))\n",
    "\n",
    "                        for i, b in enumerate(batch):\n",
    "                            state, action, reward, next_state = b[0], b[1], b[2], b[3]  # extract data from one sample\n",
    "                            current_q = q_s_a[i]  # get the Q(state) predicted before\n",
    "\n",
    "\n",
    "                            current_q[action] = reward + self._gamma * np.amax(q_s_a_d[i])  # update Q(state, action)\n",
    "                            x[i] = state\n",
    "                            y[i] = current_q  # Q(state) that includes the updated action value\n",
    "\n",
    "                        model1.train_batch(Session, x, y)  # train the NN\n",
    "                    else:\n",
    "                        states = np.array([val[0] for val in batch])  # extract states from the batch\n",
    "                        next_states = np.array([val[3] for val in batch])  # extract next states from the batch\n",
    "\n",
    "                        # prediction\n",
    "                        q_s_a = model2.predict_batch(states, Session)  # predict Q(state), for every sample\n",
    "                        q_s_a_d = model1.predict_batch(next_states, Session)  # predict Q(next_state), for every sample\n",
    "\n",
    "                        # setup training arrays\n",
    "                        x = np.zeros((len(batch), STATE_SPACE))\n",
    "                        y = np.zeros((len(batch), ACTION_SPACE))\n",
    "\n",
    "                        for i, b in enumerate(batch):\n",
    "                            state, action, reward, next_state = b[0], b[1], b[2], b[3]  # extract data from one sample\n",
    "                            current_q = q_s_a[i]  # get the Q(state) predicted before\n",
    "\n",
    "\n",
    "                            current_q[action] = reward + self._gamma * np.amax(q_s_a_d[i])  # update Q(state, action)\n",
    "                            x[i] = state\n",
    "                            y[i] = current_q  # Q(state) that includes the updated action value\n",
    "\n",
    "                        model2.train_batch(Session, x, y)  # train the NN\n",
    "\n",
    "        elif(len(memory.data)>0):\n",
    "            if(model2==None):\n",
    "                state,action,reward,next_state=memory.data[-1]\n",
    "                q_s_a = model1.predict_one(state, Session)[0]\n",
    "                q_s_a_d = model1.predict_one(next_state, Session)[0]\n",
    "                current_q = q_s_a\n",
    "                current_q[action] = reward + self._gamma * np.amax(q_s_a_d)  # update Q(state, action)\n",
    "                x=np.zeros((1, STATE_SPACE))\n",
    "                y=np.zeros((1, ACTION_SPACE))\n",
    "                x[0]=state\n",
    "                y[0]=current_q\n",
    "                model1.train_batch(Session, x, y)  # train the NN\n",
    "            else:\n",
    "                if(random.random()<=0.5):\n",
    "                    state,action,reward,next_state=memory.data[-1]\n",
    "                    q_s_a = model1.predict_one(state, Session)[0]\n",
    "                    q_s_a_d = model2.predict_one(next_state, Session)[0]\n",
    "                    current_q = q_s_a\n",
    "                    current_q[action] = reward + self._gamma * np.amax(q_s_a_d)  # update Q(state, action)\n",
    "                    x=np.zeros((1, STATE_SPACE))\n",
    "                    y=np.zeros((1, ACTION_SPACE))\n",
    "                    x[0]=state\n",
    "                    y[0]=current_q\n",
    "                    model1.train_batch(Session, x, y)  # train the NN\n",
    "                else:\n",
    "                    state,action,reward,next_state=memory.data[-1]\n",
    "                    q_s_a = model2.predict_one(state, Session)[0]\n",
    "                    q_s_a_d = model1.predict_one(next_state, Session)[0]\n",
    "                    current_q = q_s_a\n",
    "                    current_q[action] = reward + self._gamma * np.amax(q_s_a_d)  # update Q(state, action)\n",
    "                    x=np.zeros((1, STATE_SPACE))\n",
    "                    y=np.zeros((1, ACTION_SPACE))\n",
    "                    x[0]=state\n",
    "                    y[0]=current_q\n",
    "                    model2.train_batch(Session, x, y)\n",
    "\n",
    "\n",
    "    # NOT COMPLETE, prediction part #\n",
    "    def _choose_action(self, state, epsilon, model, Session):\n",
    "        if random.random() < epsilon:\n",
    "            return random.randint(0, ACTION_SPACE - 1) # random action\n",
    "        else:\n",
    "            return np.argmax(model.predict_one(state, Session)) # the best action given the current state\n",
    "\n",
    "\n",
    "    # SET IN SUMO THE CORRECT YELLOW PHASE\n",
    "    def _set_yellow_phase(self, old_action):\n",
    "        yellow_phase = old_action * 2 + 1 # obtain the yellow phase code, based on the old action\n",
    "        traci.trafficlight.setPhase(\"TL\", yellow_phase)\n",
    "\n",
    "    # SET IN SUMO A GREEN PHASE\n",
    "    def _set_green_phase(self, action_number):\n",
    "        if action_number == 0:\n",
    "            traci.trafficlight.setPhase(\"TL\", PHASE_NS_GREEN)\n",
    "        elif action_number == 1:\n",
    "            traci.trafficlight.setPhase(\"TL\", PHASE_NSL_GREEN)\n",
    "        elif action_number == 2:\n",
    "            traci.trafficlight.setPhase(\"TL\", PHASE_EW_GREEN)\n",
    "        elif action_number == 3:\n",
    "            traci.trafficlight.setPhase(\"TL\", PHASE_EWL_GREEN)\n",
    "\n",
    "    # RETRIEVE THE STATS OF THE SIMULATION FOR ONE SINGLE STEP\n",
    "    def _get_stats(self):\n",
    "        halt_N = traci.edge.getLastStepHaltingNumber(\"N2TL\")\n",
    "        halt_S = traci.edge.getLastStepHaltingNumber(\"S2TL\")\n",
    "        halt_E = traci.edge.getLastStepHaltingNumber(\"E2TL\")\n",
    "        halt_W = traci.edge.getLastStepHaltingNumber(\"W2TL\")\n",
    "        intersection_queue = halt_N + halt_S + halt_E + halt_W\n",
    "        return intersection_queue\n",
    "\n",
    "    # RETRIEVE THE STATE OF THE INTERSECTION FROM SUMO\n",
    "    def _get_state(self):\n",
    "        state = np.zeros(STATE_SPACE)\n",
    "\n",
    "        for veh_id in traci.vehicle.getIDList():\n",
    "            lane_pos = traci.vehicle.getLanePosition(veh_id)\n",
    "            lane_id = traci.vehicle.getLaneID(veh_id)\n",
    "            lane_pos = 750 - lane_pos  # inversion of lane pos, so if the car is close to TL, lane_pos = 0\n",
    "            lane_group = -1  # just dummy initialization\n",
    "            valid_car = False  # flag for not detecting cars crossing the intersection or driving away from it\n",
    "\n",
    "            # distance in meters from the TLS -> mapping into cells\n",
    "            if lane_pos < 7:\n",
    "                lane_cell = 0\n",
    "            elif lane_pos < 14:\n",
    "                lane_cell = 1\n",
    "            elif lane_pos < 21:\n",
    "                lane_cell = 2\n",
    "            elif lane_pos < 28:\n",
    "                lane_cell = 3\n",
    "            elif lane_pos < 40:\n",
    "                lane_cell = 4\n",
    "            elif lane_pos < 60:\n",
    "                lane_cell = 5\n",
    "            elif lane_pos < 100:\n",
    "                lane_cell = 6\n",
    "            elif lane_pos < 160:\n",
    "                lane_cell = 7\n",
    "            elif lane_pos < 400:\n",
    "                lane_cell = 8\n",
    "            elif lane_pos <= 750:\n",
    "                lane_cell = 9\n",
    "\n",
    "            # finding the lane where the car is located - _3 are the \"turn left only\" lanes\n",
    "            if lane_id == \"W2TL_0\" or lane_id == \"W2TL_1\" or lane_id == \"W2TL_2\":\n",
    "                lane_group = 0\n",
    "            elif lane_id == \"W2TL_3\":\n",
    "                lane_group = 1\n",
    "            elif lane_id == \"N2TL_0\" or lane_id == \"N2TL_1\" or lane_id == \"N2TL_2\":\n",
    "                lane_group = 2\n",
    "            elif lane_id == \"N2TL_3\":\n",
    "                lane_group = 3\n",
    "            elif lane_id == \"E2TL_0\" or lane_id == \"E2TL_1\" or lane_id == \"E2TL_2\":\n",
    "                lane_group = 4\n",
    "            elif lane_id == \"E2TL_3\":\n",
    "                lane_group = 5\n",
    "            elif lane_id == \"S2TL_0\" or lane_id == \"S2TL_1\" or lane_id == \"S2TL_2\":\n",
    "                lane_group = 6\n",
    "            elif lane_id == \"S2TL_3\":\n",
    "                lane_group = 7\n",
    "\n",
    "            if lane_group >= 1 and lane_group <= 7:\n",
    "                veh_position = int(str(lane_group) + str(lane_cell))  # composition of the two postion ID to create a number in interval 0-79\n",
    "                valid_car = True\n",
    "            elif lane_group == 0:\n",
    "                veh_position = lane_cell\n",
    "                valid_car = True\n",
    "\n",
    "            if valid_car:\n",
    "                state[veh_position] = 1  # write the position of the car veh_id in the state array\n",
    "\n",
    "        return state\n",
    "\n",
    "    # SAVE THE STATS OF THE EPISODE TO PLOT THE GRAPHS AT THE END OF THE SESSION\n",
    "    def _save_stats(self, tot_neg_reward):\n",
    "        self._reward_store.append(tot_neg_reward)  # how much negative reward in this episode\n",
    "        self._cumulative_wait_store.append(self._sum_intersection_queue)  # total number of seconds waited by cars in this episode\n",
    "        self._avg_intersection_queue_store.append(self._sum_intersection_queue / MAX_STEPS_PER_EPS)  # average number of queued cars per step, in this episode\n",
    "\n",
    "    @property\n",
    "    def reward_store(self):\n",
    "        return self._reward_store\n",
    "\n",
    "    @property\n",
    "    def cumulative_wait_store(self):\n",
    "        return self._cumulative_wait_store\n",
    "\n",
    "    @property\n",
    "    def avg_intersection_queue_store(self):\n",
    "        return self._avg_intersection_queue_store\n",
    "\n",
    "# PLOT AND SAVE THE STATS ABOUT THE SESSION\n",
    "def graphs(sim_runner_dqn,sim_runner_ddqn,sim_runner_dqn_batch,sim_runner_ddqn_batch ,total_episodes):\n",
    "    x=list()\n",
    "    for i in range(total_episodes):\n",
    "        x.append(i+1)\n",
    "\n",
    "    plt.plot(x,sim_runner_dqn_batch.reward_store,'k')\n",
    "    plt.plot(x,sim_runner_ddqn_batch.reward_store,'y')\n",
    "    yellow_patch = mpatches.Patch(color='yellow', label='DDQN_Batch')\n",
    "    black_patch = mpatches.Patch(color='black', label='DQN_Batch')\n",
    "    plt.legend(handles=[yellow_patch,black_patch],loc=1)\n",
    "    plt.xlabel('episodes')\n",
    "    plt.ylabel('total_reward')\n",
    "    plt.show()\n",
    "    \n",
    "    plt.plot(x,sim_runner_dqn_batch.cumulative_wait_store,'k')\n",
    "    plt.plot(x,sim_runner_ddqn_batch.cumulative_wait_store,'y')\n",
    "    yellow_patch = mpatches.Patch(color='yellow', label='DDQN_Batch')\n",
    "    black_patch = mpatches.Patch(color='black', label='DQN_Batch')\n",
    "    plt.legend(handles=[yellow_patch,black_patch],loc=1)\n",
    "    plt.xlabel('episodes')\n",
    "    plt.ylabel('total waiting time')\n",
    "    plt.show()\n",
    "    \n",
    "    plt.plot(x,sim_runner_dqn_batch.avg_intersection_queue_store,'k')\n",
    "    plt.plot(x,sim_runner_ddqn_batch.avg_intersection_queue_store,'y')\n",
    "    yellow_patch = mpatches.Patch(color='yellow', label='DDQN_batch')\n",
    "    black_patch = mpatches.Patch(color='black', label='DQN_batch')\n",
    "    plt.legend(handles=[yellow_patch,black_patch],loc=1)\n",
    "    plt.xlabel('episodes')\n",
    "    plt.ylabel('average queue length')\n",
    "    plt.show()\n",
    "    \n",
    "    plt.plot(x,sim_runner_dqn_batch.reward_store,'k')\n",
    "    plt.plot(x,sim_runner_dqn.reward_store,'y')\n",
    "    yellow_patch = mpatches.Patch(color='yellow', label='Simple DQN')\n",
    "    black_patch = mpatches.Patch(color='black', label='DQN_Batch')\n",
    "    plt.legend(handles=[yellow_patch,black_patch],loc=1)\n",
    "    plt.xlabel('episodes')\n",
    "    plt.ylabel('total_reward')\n",
    "    plt.show()\n",
    "    \n",
    "    plt.plot(x,sim_runner_ddqn_batch.reward_store,'k')\n",
    "    plt.plot(x,sim_runner_ddqn.reward_store,'y')\n",
    "    yellow_patch = mpatches.Patch(color='yellow', label='Simple DDQN')\n",
    "    black_patch = mpatches.Patch(color='black', label='DDQN_Batch')\n",
    "    plt.legend(handles=[yellow_patch,black_patch],loc=1)\n",
    "    plt.xlabel('episodes')\n",
    "    plt.ylabel('total_reward')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if SHOW_GUI:\n",
    "    sumoBinary = checkBinary('sumo-gui')\n",
    "else:\n",
    "    sumoBinary = checkBinary('sumo')\n",
    "\n",
    "_traffic_generator = traffic_generator.TrafficGenerator(MAX_STEPS_PER_EPS, NO_OF_CARS)\n",
    "sumo_cmd = [sumoBinary, \"-c\", \"intersection/tlcs_config_train.sumocfg\", \"--no-step-log\", \"true\", \"--waiting-time-memory\", str(MAX_STEPS_PER_EPS)]\n",
    "\n",
    "model1 = Model(STATE_SPACE, ACTION_SPACE, BATCH)\n",
    "model2 = Model(STATE_SPACE, ACTION_SPACE, BATCH)\n",
    "model3 = Model(STATE_SPACE, ACTION_SPACE, BATCH)\n",
    "\n",
    "memory1 = memory.Memory(MEMORY)\n",
    "memory2 = memory.Memory(MEMORY)\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "\n",
    "#simple ddqn\n",
    "with tf.Session() as Session:\n",
    "    Session.run(model1.var_init)\n",
    "    Session.run(model2.var_init)\n",
    "    simulator_ddqn_simple = Simulator(_traffic_generator, MAX_STEPS_PER_EPS, 0.75)\n",
    "    episode_count = 0\n",
    "    port = 5000\n",
    "    while episode_count < NO_OF_EPISODES:\n",
    "        epsilon = 0.1\n",
    "        print('----- Episode {} of {}'.format(episode_count + 1, NO_OF_EPISODES))\n",
    "        start = timeit.default_timer()\n",
    "        simulator_ddqn_simple.run(Session, memory1, model1, model2, epsilon, port, sumo_cmd)  # run the simulation\n",
    "        stop = timeit.default_timer()\n",
    "        print('Time: ', round(stop - start, 1))\n",
    "        episode_count += 1\n",
    "        port += 1\n",
    "\n",
    "#simple dqn\n",
    "with tf.Session() as Session:\n",
    "    Session.run(model3.var_init)\n",
    "    simulator_dqn_simple = Simulator(_traffic_generator, MAX_STEPS_PER_EPS, 0.75)\n",
    "    episode_count = 0\n",
    "    port = 5000\n",
    "    while episode_count < NO_OF_EPISODES:\n",
    "        epsilon = 0.1\n",
    "        print('----- Episode {} of {}'.format(episode_count + 1, NO_OF_EPISODES))\n",
    "        start = timeit.default_timer()\n",
    "        simulator_dqn_simple.run(Session, memory2, model1, None, epsilon, port, sumo_cmd)  # run the simulation\n",
    "        stop = timeit.default_timer()\n",
    "        print('Time: ', round(stop - start, 1))\n",
    "        episode_count += 1\n",
    "        port += 1\n",
    "\n",
    "\n",
    "model1 = Model(STATE_SPACE, ACTION_SPACE, BATCH)\n",
    "model2 = Model(STATE_SPACE, ACTION_SPACE, BATCH)\n",
    "model3 = Model(STATE_SPACE, ACTION_SPACE, BATCH)\n",
    "\n",
    "memory1 = memory.Memory(MEMORY)\n",
    "memory2 = memory.Memory(MEMORY)\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "#batch DDQN\n",
    "with tf.Session() as Session:\n",
    "    Session.run(model1.var_init)\n",
    "    Session.run(model2.var_init)\n",
    "    simulator_ddqn = Simulator(_traffic_generator, MAX_STEPS_PER_EPS, 0.75)\n",
    "    episode_count = 0\n",
    "    port = 7000\n",
    "\n",
    "    while episode_count < NO_OF_EPISODES:\n",
    "        print(\"Episode \" + str(episode_count) + \" started\")\n",
    "        epsilon = 0.1\n",
    "        start_time = timeit.default_timer()\n",
    "        simulator_ddqn.run(Session, memory1, model1, model2, epsilon, port, sumo_cmd)\n",
    "        end_time = timeit.default_timer()\n",
    "        print(\"Time taken: \", str(end_time - start_time))\n",
    "        episode_count += 1\n",
    "        port += 1\n",
    "\n",
    "    print(\"Simulation Done\")\n",
    "\n",
    "# batch DQN\n",
    "with tf.Session() as Session:\n",
    "    Session.run(model3.var_init)\n",
    "    simulator_dqn = Simulator(_traffic_generator, MAX_STEPS_PER_EPS, 0.75)\n",
    "    episode_count = 0\n",
    "    port = 14000\n",
    "\n",
    "    while episode_count < NO_OF_EPISODES:\n",
    "        print(\"Episode \" + str(episode_count) + \" started\")\n",
    "        epsilon = 0.1\n",
    "        start_time = timeit.default_timer()\n",
    "        simulator_dqn.run(Session, memory2, model3, None, epsilon, port, sumo_cmd)\n",
    "        end_time = timeit.default_timer()\n",
    "        print(\"Time taken: \", str(end_time - start_time))\n",
    "        episode_count += 1\n",
    "        port += 1\n",
    "\n",
    "    print(\"Simulation Done\")\n",
    "\n",
    "\n",
    "graphs(simulator_dqn_simple,simulator_ddqn_simple,simulator_dqn,simulator_ddqn,NO_OF_EPISODES)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
